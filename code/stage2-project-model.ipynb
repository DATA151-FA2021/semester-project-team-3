{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import matplotlib.pyplot as plt\n", "import numpy as np\n", "import pandas as pd\n", "import seaborn as sns\n", "import random as rand\n", "from sklearn.model_selection import (\n", "    train_test_split,\n", "    StratifiedShuffleSplit,\n", "    cross_val_score,\n", ")\n", "from sklearn.linear_model import LogisticRegression\n", "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n", "from sklearn import metrics\n", "from sklearn.metrics import (\n", "    accuracy_score,\n", "    recall_score,\n", "    precision_score,\n", "    f1_score,\n", "    confusion_matrix,\n", "    roc_curve,\n", "    roc_auc_score,\n", ")\n", "from sklearn.ensemble import RandomForestClassifier\n", "from imblearn.over_sampling import RandomOverSampler, SMOTE\n", "from scipy.stats import chi2_contingency\n", "from statsmodels.graphics.mosaicplot import mosaic"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df = pd.read_csv(\"../data/student_por_mod.csv\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df[\"failures\"] = df[\"failures\"].map({0: \"no_fail\", 1: \"fail\", 2: \"fail\", 3: \"fail\"})"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df[\"failures\"] = np.where(df.failures == \"fail\", 1, 0)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Obtained through feature importance graph, the variables that gave the highest<br>\n", "score."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["predictors = [\"G3\", \"age\", \"G1\", \"absences\", \"Medu\", \"goout\", \"Fedu\"]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["x = pd.get_dummies(df[predictors], drop_first=True)\n", "print(x.head())"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["y = df[\"failures\"]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["rand.seed(10)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.2, stratify=y)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def runModel(model, xt, yt, xv, yv):\n", "    model.fit(xt, yt)\n", "    y_pred = model.predict(xv)\n", "    y_pred_prob = model.predict_proba(xv)[:, 1]\n", "    print(y_pred_prob[0:10])  # first coloumn is prob of negative class (fail)\n", "    labels = np.unique(yv)\n", "    cm = confusion_matrix(yv, y_pred_prob > 0.2, labels=labels)\n", "    print(pd.DataFrame(cm, index=labels, columns=labels))\n", "    print(f\"{accuracy_score.__name__} : {accuracy_score(yv, y_pred_prob > 0.2)}\")\n", "    for func in [recall_score, precision_score, f1_score]:\n", "        print(f\"{func.__name__} :  {func(yv, y_pred_prob > 0.2, average = 'weighted')}\")\n\n", "    # print classification report\n", "    print(metrics.classification_report(yv, y_pred))\n", "    # calculate scores & Extracting probabilities\n", "    auc = roc_auc_score(yv, pd.Series(model.predict_proba(xv)[:, 1]))\n", "    # summarize scores\n", "    print(f\"ROC AUC : {auc:.3f}\")\n", "    plt.figure()\n", "    m_fpr, m_tpr, _ = roc_curve(yv, pd.Series(y_pred_prob))\n", "    plt.plot(m_fpr, m_tpr, color=\"darkorange\", lw=3)\n", "    plt.xlabel(\"False Positive Rate\")\n", "    plt.ylabel(\"True Positive Rate\")\n", "    plt.savefig(f\"{model}.png\", dpi=150, bbox_inches=\"tight\")\n\n", "    # Cross-validation\n", "    scores = cross_val_score(model, xt, yt, cv=10, scoring=\"recall\")\n", "    print(f\"cross validation : {scores}\\nmean : {scores.mean()}\")\n\n", "    # Cross-validation splitter as a cv parameter\n", "    shuffle_split = StratifiedShuffleSplit(\n", "        test_size=0.2,\n", "        n_splits=10,\n", "        random_state=123,\n", "    )\n", "    scores = cross_val_score(model, xt, yt, cv=shuffle_split, scoring=\"recall\")\n", "    print(f\"cross validation : {scores}\\nmean : {scores.mean()}\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["First Model: Decision Tree"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["d_tree = DecisionTreeClassifier(max_depth=3, random_state=0)\n", "runModel(d_tree, x_train, y_train, x_val, y_val)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Second Model: Random Forest"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["r_forest = RandomForestClassifier(n_estimators=500, random_state=7)\n", "runModel(r_forest, x_train, y_train, x_val, y_val)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Plotting variable importance for Random Forest"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(5, 10))\n", "sorted_idx = r_forest.feature_importances_.argsort()\n", "plt.barh(x.columns[0:][sorted_idx], r_forest.feature_importances_[sorted_idx])\n", "plt.xlabel(f\"{r_forest} Feature Importance\")\n", "plt.title(f\"{r_forest} Feature Imortance by Group 3\")\n", "plt.savefig(f\"{r_forest}_feature_importance.png\", dpi=150, bbox_inches=\"tight\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Third Model: Random Forest with SMOTE undersampling"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["sm = SMOTE(sampling_strategy=\"minority\", random_state=2)\n", "x_train_res, y_train_res = sm.fit_resample(x_train, y_train)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["RF = RandomForestClassifier(random_state=2)\n", "runModel(RF, x_train_res, y_train_res, x_val, y_val)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Fourth Model: Logistic Regression"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df = df.drop(\n", "    [\n", "        \"G1\",\n", "        \"G2\",\n", "        \"G3\",\n", "        \"absences\",\n", "        \"age\",\n", "        \"Medu\",\n", "        \"Fedu\",\n", "        \"traveltime\",\n", "        \"studytime\",\n", "        \"famrel\",\n", "        \"freetime\",\n", "        \"goout\",\n", "        \"Walc\",\n", "        \"Dalc\",\n", "        \"health\",\n", "    ],\n", "    axis=1,\n", ")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Using train_test_split function to split the dataset"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.3, stratify=y, random_state=1234)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Chi Square test of Independence"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["for col in df.columns[1:]:\n", "    crosstab = pd.crosstab(df[col], df[\"failures\"], margins=True)\n", "    stat, p, dof, expected = chi2_contingency(crosstab)\n", "    print(\"P value of Chi Square between failures and\", col, \"is\", p)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["mosaic(data=df, index=[\"reason\", \"failures\"])\n", "plt.title(\"Mosaic Plot of Reason and Failures by Group 3\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(df[\"failures\"].value_counts())"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["predictors = df.columns[[7, 8, 15]]  # Choose Predictors from the dataset"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["x = pd.get_dummies(df[predictors], drop_first=True)\n", "x.head()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["lr_clf = LogisticRegression()\n", "runModel(lr_clf, x_train, y_train, x_val, y_val)"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}